[2022-04-21 05:22:20,534] [WARNING] [partition_parameters.py:53:<module>] unable to find torch.distributed._all_gather_base. will fall back to torch.distributed.all_gather which will result in suboptimal performance. please consider upgrading your pytorch installation.
/home2/gaobofei/anaconda3/lib/python3.8/site-packages/apex-0.1-py3.8.egg/apex/pyprof/__init__.py:5: FutureWarning: pyprof will be removed by the end of June, 2022
  warnings.warn("pyprof will be removed by the end of June, 2022", FutureWarning)
/home2/gaobofei/anaconda3/lib/python3.8/site-packages/apex-0.1-py3.8.egg/apex/pyprof/__init__.py:5: FutureWarning: pyprof will be removed by the end of June, 2022
  warnings.warn("pyprof will be removed by the end of June, 2022", FutureWarning)
/home2/gaobofei/anaconda3/lib/python3.8/site-packages/apex-0.1-py3.8.egg/apex/pyprof/__init__.py:5: FutureWarning: pyprof will be removed by the end of June, 2022
  warnings.warn("pyprof will be removed by the end of June, 2022", FutureWarning)
/home2/gaobofei/anaconda3/lib/python3.8/site-packages/apex-0.1-py3.8.egg/apex/pyprof/__init__.py:5: FutureWarning: pyprof will be removed by the end of June, 2022
  warnings.warn("pyprof will be removed by the end of June, 2022", FutureWarning)
[2022-04-21 05:22:23,428] [WARNING] [partition_parameters.py:53:<module>] unable to find torch.distributed._all_gather_base. will fall back to torch.distributed.all_gather which will result in suboptimal performance. please consider upgrading your pytorch installation.
[2022-04-21 05:22:23,499] [WARNING] [partition_parameters.py:53:<module>] unable to find torch.distributed._all_gather_base. will fall back to torch.distributed.all_gather which will result in suboptimal performance. please consider upgrading your pytorch installation.
[2022-04-21 05:22:23,541] [WARNING] [partition_parameters.py:53:<module>] unable to find torch.distributed._all_gather_base. will fall back to torch.distributed.all_gather which will result in suboptimal performance. please consider upgrading your pytorch installation.
[2022-04-21 05:22:23,567] [WARNING] [partition_parameters.py:53:<module>] unable to find torch.distributed._all_gather_base. will fall back to torch.distributed.all_gather which will result in suboptimal performance. please consider upgrading your pytorch installation.
loading configuration file https://huggingface.co/fnlp/cpt-base/resolve/main/config.json from cache at /home2/gaobofei/.cache/huggingface/transformers/5e8018bdccae5c0e3d7464fc230a05a1c857e9e0ef5bc2d100680009e00e76f0.da6ee5ba6f5e82cef9e75d75e30af47c6376689b83a1d3e1906d4e867f9309a1
Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 101,
  "classif_dropout": 0.1,
  "classifier_dropout": 0.1,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 2,
  "decoder_start_token_id": 102,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 12,
  "eos_token_id": 102,
  "forced_eos_token_id": 102,
  "gradient_checkpointing": false,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "scale_embedding": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "transformers_version": "4.4.2",
  "use_cache": true,
  "vocab_size": 21128
}

loading file https://huggingface.co/fnlp/cpt-base/resolve/main/vocab.txt from cache at /home2/gaobofei/.cache/huggingface/transformers/1e78823f936f9b7fd284d7edeb9f15bd80ab03c4d935324bc93d17569404c6a8.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb
loading file https://huggingface.co/fnlp/cpt-base/resolve/main/added_tokens.json from cache at None
loading file https://huggingface.co/fnlp/cpt-base/resolve/main/special_tokens_map.json from cache at /home2/gaobofei/.cache/huggingface/transformers/0c554635712016feb81785acc5036893453dd9c880c08e20e9a070727527d833.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d
loading file https://huggingface.co/fnlp/cpt-base/resolve/main/tokenizer_config.json from cache at /home2/gaobofei/.cache/huggingface/transformers/a4f787d368f75b4d1e1e1f3e16eb2bcac3d43fd469f5e3e5fffed90401e342e0.4930bdcbc6f75dead7cdeadc249fdb55dcb3cd75bdcee68ee5fcd8aeb6e6e359
loading file https://huggingface.co/fnlp/cpt-base/resolve/main/tokenizer.json from cache at None
Traceback (most recent call last):
  File "/home2/gaobofei/anaconda3/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home2/gaobofei/anaconda3/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home2/gaobofei/anaconda3/lib/python3.8/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home2/gaobofei/anaconda3/lib/python3.8/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home2/gaobofei/anaconda3/lib/python3.8/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home2/gaobofei/anaconda3/bin/python', '-u', 'query_gen_notrainer.py', '--model_path', 'fnlp/cpt-base', '--output_dir', 'output/query_baseline', '--train_file', 'DuSinc/dialogue/train.csv', '--validation_file', 'DuSinc/dialogue/dev.csv', '--test_file', 'DuSinc/dialogue/test.csv', '--mode', 'train', '--per_device_batch_size', '5', '--experiment_name', 'query_gen', '--scenario_name', 'batch_size4*5,baseline_input', '--num_train_epochs', '10']' died with <Signals.SIGKILL: 9>.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 2316766
Killing subprocess 2316767
Killing subprocess 2316768
Killing subprocess 2316769
/home2/gaobofei/anaconda3/lib/python3.8/site-packages/apex-0.1-py3.8.egg/apex/pyprof/__init__.py:5: FutureWarning: pyprof will be removed by the end of June, 2022
  warnings.warn("pyprof will be removed by the end of June, 2022", FutureWarning)
Traceback (most recent call last):
  File "/home2/gaobofei/anaconda3/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/home2/gaobofei/anaconda3/lib/python3.8/site-packages/accelerate/commands/accelerate_cli.py", line 43, in main
    args.func(args)
  File "/home2/gaobofei/anaconda3/lib/python3.8/site-packages/accelerate/commands/launch.py", line 468, in launch_command
    multi_gpu_launcher(args)
  File "/home2/gaobofei/anaconda3/lib/python3.8/site-packages/accelerate/commands/launch.py", line 228, in multi_gpu_launcher
    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)
subprocess.CalledProcessError: Command '['/home2/gaobofei/anaconda3/bin/python', '-m', 'torch.distributed.launch', '--use_env', '--nproc_per_node', '4', 'query_gen_notrainer.py', '--model_path', 'fnlp/cpt-base', '--output_dir', 'output/query_baseline', '--train_file', 'DuSinc/dialogue/train.csv', '--validation_file', 'DuSinc/dialogue/dev.csv', '--test_file', 'DuSinc/dialogue/test.csv', '--mode', 'train', '--per_device_batch_size', '5', '--experiment_name', 'query_gen', '--scenario_name', 'batch_size4*5,baseline_input', '--num_train_epochs', '10']' returned non-zero exit status 1.
